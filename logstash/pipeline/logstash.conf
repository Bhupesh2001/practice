input {
  # TCP input for direct log shipping from applications
  tcp {
    port => 5000
    codec => json_lines
    tags => ["tcp"]
  }

  # UDP input for high-throughput logging
  udp {
    port => 5000
    codec => json_lines
    tags => ["udp"]
  }

  # Beats input (for Filebeat)
  beats {
    port => 5044
    tags => ["beats"]
  }
}

filter {
  # Parse JSON logs
  if [message] =~ /^\{.*\}$/ {
    json {
      source => "message"
      target => "parsed"
    }
  }

  # Extract service name from tags or logger name
  if [logger_name] {
    grok {
      match => { "logger_name" => "(?<service_name>[^.]+)\." }
    }
  }

  # Add timestamp
  date {
    match => [ "timestamp", "ISO8601", "yyyy-MM-dd HH:mm:ss.SSS" ]
    target => "@timestamp"
  }

  # Categorize log levels
  if [level] {
    mutate {
      uppercase => [ "level" ]
    }
  }

  # Extract correlation ID if present
  if [mdc] {
    mutate {
      add_field => {
        "correlation_id" => "%{[mdc][correlationId]}"
        "user_id" => "%{[mdc][userId]}"
        "request_id" => "%{[mdc][requestId]}"
      }
    }
  }

  # Add service-specific tags
  if [service_name] == "auth-service" {
    mutate {
      add_tag => ["authentication", "security"]
    }
  } else if [service_name] == "user-service" {
    mutate {
      add_tag => ["user-management", "profile"]
    }
  } else if [service_name] == "gateway-service" {
    mutate {
      add_tag => ["api-gateway", "routing"]
    }
  }

  # Parse stack traces
  if [stack_trace] {
    mutate {
      add_tag => ["error", "exception"]
    }
  }

  # Remove unnecessary fields
  mutate {
    remove_field => [ "host", "port", "@version" ]
  }
}

output {
  # Output to Elasticsearch with better index naming
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "microservices-%{[service]}-%{+YYYY.MM.dd}"
    document_type => "_doc"
  }

  # Output to console for debugging (remove in production)
  stdout {
    codec => rubydebug
  }
}